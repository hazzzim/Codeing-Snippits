{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKPE4D24MZGL"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "###import neccesary libraries ###\n",
        "#################################\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "import os\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.python.client import device_lib\n",
        "from pickle import load\n",
        "from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error\n",
        "import random\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from keras import regularizers\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
        "import math\n",
        "from keras import optimizers\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLpR_M0-MZGM"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#######Storing Data   ########\n",
        "##############################\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(12345)\n",
        "cwd = os.getcwd()\n",
        "os.chdir(cwd)                                   #Working directory\n",
        "filename = 'Puck_trained.sav'                   #File name\n",
        "best_model = 'best_model.h5'                    #Early stopping for best model selection\n",
        "def huber_loss(y_true, y_pred):\n",
        "        return tf.losses.huber_loss(y_true,y_pred)\n",
        "    \n",
        "def mean_absolute_error_180(y_true, y_pred):\n",
        "    delta = K.minimum(K.minimum(K.abs(y_pred - y_true),\n",
        "                              K.abs(y_pred - (180+y_true))),\n",
        "                              K.abs(y_true - (180+y_pred)))\n",
        "    return K.mean(delta)\n",
        "\n",
        "def absolute_error_180(y_true, y_pred):\n",
        "    delta = K.minimum(K.minimum(K.abs(y_pred - y_true),\n",
        "                              K.abs(y_pred - (180+y_true))),\n",
        "                              K.abs(y_true - (180+y_pred)))\n",
        "    return delta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mean_sqared_error_180(y_true, y_pred):\n",
        "    delta = K.minimum(K.minimum(K.abs(y_pred - y_true),\n",
        "                              K.abs(y_pred - (180+y_true))),\n",
        "                              K.abs(y_true - (180+y_pred)))\n",
        "    return K.mean(K.square(delta))\n",
        "\n",
        "def mean_sqared_squared_error_180(y_true, y_pred):\n",
        "    delta = K.minimum(K.minimum(K.abs(y_pred - y_true),\n",
        "                              K.abs(y_pred - (180+y_true))),\n",
        "                              K.abs(y_true - (180+y_pred)))\n",
        "    return K.mean(K.square(K.square(delta)))\n",
        "\n",
        "def mean_absolute_error_eval(y_test, y_pred):\n",
        "    error = np.empty([len(y_test), 1])\n",
        "    for i in range(len(y_test)):\n",
        "        error[i] = min(min(np.abs(y_pred[i] - y_test[i]),\n",
        "                                  np.abs(y_pred[i] - (180+y_test[i]))),\n",
        "                                  np.abs(y_test[i] - (180+y_pred[i])))\n",
        "    return np.mean(error)\n",
        "\n",
        "\n",
        "\n",
        "def rmse_360(y_true, y_pred):\n",
        "    return K.sqrt(mean_squared_error_360(y_true, y_pred))\n",
        "\n",
        "def atan2(x, y, epsilon=1.0e-12):\n",
        "    x = tf.where(tf.equal(x, 0.0), x+epsilon, x)\n",
        "    y = tf.where(tf.equal(y, 0.0), y+epsilon, y)    \n",
        "    angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\n",
        "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n",
        "    angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n",
        "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n",
        "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n",
        "    angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\n",
        "    return angle\n",
        "\n",
        "def rmse_360_2(y_true, y_pred):\n",
        "    return K.mean(K.abs(atan2(K.sin(y_true - y_pred), K.cos(y_true - y_pred))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amEwirK-MZGN"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#######  User inputs  ########\n",
        "##############################\n",
        "\n",
        "Sigma_min = -35                              #MPa\n",
        "Sigma_max = 60                               #MPa\n",
        "R_t_c = 240                                  #MPa\n",
        "R_t_t = 67                                   #MPa\n",
        "R_tp = 62                                    #MPa\n",
        "R_tt = 47.4                                  #MPa\n",
        "p_tt_c = 0.25\n",
        "p_tt_t = 0.25\n",
        "p_tp_t = 0.35\n",
        "p_tp_c = 0.30\n",
        "angle_steps = 180                            #number of angle steps (180 = 1° Schritte)\n",
        "dataset_size = 10000                         #data size\n",
        "Test_split_size = 0.3                        \n",
        "Validation_split_size = 0.1                  #Wie viel Prozent des trainings Datasets sollten zum validieren verwendet werden\n",
        "First_layer = 60                             #number of neurons in the layers\n",
        "Secound_layer = 60                           \n",
        "Third_layer = 60                             \n",
        "Fourth_layer = 60                            \n",
        "Fith_layer = 60                              \n",
        "Sixth_layer = 60                             \n",
        "loss_function = absolute_error_180           #User defined loss function\n",
        "metric_function = 'mae'                      #metrics to measure loss\n",
        "optimizer_function = 'adam'                  #Optimizer \n",
        "batch_size = 32                              #batch size\n",
        "number_of_epochs = 500                       \n",
        "kernel_initializer = 'he_uniform'\n",
        "activation = 'linear'                          \n",
        "early_stopping_monitoring = 'val_loss'   \n",
        "early_stopping_patience = 10           \n",
        "kernel_regularizer = regularizers.l1(0.01)    \n",
        "activity_regularizer = None #regularizers.l1(0.01)                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQG8d4vBMZGU"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "########Daten generation########\n",
        "##############################\n",
        "\n",
        "#angle generation\n",
        "winkel = np.linspace(0,180, angle_steps)\n",
        "\n",
        "R_tt_A = R_t_c/(2*(1+p_tt_c))\n",
        "\n",
        "#Stress generation\n",
        "sigma_true = np.empty([5, dataset_size])\n",
        "\n",
        "sigma_n = np.empty([len(winkel), dataset_size])\n",
        "teta_nt = np.empty([len(winkel), dataset_size])\n",
        "teta_nl = np.empty([len(winkel), dataset_size])\n",
        "\n",
        "f_e = np.empty([len(winkel), dataset_size])\n",
        "\n",
        "cos_psi = np.empty([len(winkel), dataset_size])\n",
        "sin_psi = np.empty([len(winkel), dataset_size])\n",
        "\n",
        "p_R_druck = np.empty([len(winkel), dataset_size])\n",
        "p_R_zug = np.empty([len(winkel), dataset_size])\n",
        "\n",
        "indizes = list()\n",
        "failure_angle = np.empty([1, dataset_size])\n",
        "\n",
        "for i in range(0, dataset_size):\n",
        "    sigma_true[0, i] = random.uniform(Sigma_min, Sigma_max)\n",
        "    sigma_true[1, i] = random.uniform(Sigma_min, Sigma_max)\n",
        "    sigma_true[2, i] = random.uniform(Sigma_min, Sigma_max)\n",
        "    sigma_true[3, i] = random.uniform(Sigma_min, Sigma_max)\n",
        "    sigma_true[4, i] = random.uniform(Sigma_min, Sigma_max)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "for i in range(dataset_size):\n",
        "    for j in range(len(winkel)):\n",
        "        sigma_n[j, i] =  sigma_true[0, i]*(math.cos(math.radians(winkel[j])))**2 + sigma_true[1, i]*(math.sin(math.radians(winkel[j])))**2 + 2* sigma_true[2, i]*math.sin(math.radians(winkel[j]))*math.cos(math.radians(winkel[j]))\n",
        "        teta_nt[j, i] = -sigma_true[0, i]*math.sin(math.radians(winkel[j]))*math.cos(math.radians(winkel[j])) + sigma_true[1, i]*math.sin(math.radians(winkel[j]))*math.cos(math.radians(winkel[j])) + sigma_true[2, i]*((math.cos(math.radians(winkel[j])))**2 - (math.sin(math.radians(winkel[j])))**2)\n",
        "        teta_nl[j, i] = sigma_true[3, i]*math.sin(math.radians(winkel[j])) + sigma_true[4, i]*math.cos(math.radians(winkel[j]))\n",
        "        cos_psi[j, i] = teta_nt[j, i]**2/(teta_nt[j, i]**2 + teta_nl[j, i]**2)\n",
        "        sin_psi[j, i] = teta_nl[j, i]**2/(teta_nt[j, i]**2 + teta_nl[j, i]**2)\n",
        "        p_R_druck[j, i] = (p_tt_c/R_tt_A)*cos_psi[j, i] + (p_tp_c/R_tp)*sin_psi[j, i]\n",
        "        p_R_zug[j, i] = (p_tt_t/R_tt_A)*cos_psi[j, i] + (p_tp_t/R_tp)*sin_psi[j, i]\n",
        "\n",
        "        \n",
        "for i in range(dataset_size):\n",
        "    for j in range(len(winkel)):\n",
        "        if sigma_n[j, i] >= 0:\n",
        "            f_e[j, i] = math.sqrt(((1/R_t_t - p_R_zug[j, i])*sigma_n[j, i])**2 + (teta_nt[j, i]/R_tt_A)**2 + (teta_nl[j, i]/R_tp)**2) + p_R_zug[j, i]*sigma_n[j, i]\n",
        "        else:\n",
        "            f_e[j, i] = math.sqrt((teta_nt[j, i]/R_tt_A)**2 + (teta_nl[j, i]/R_tp)**2 + (p_R_druck[j, i]*sigma_n[j, i])**2) + p_R_druck[j, i] * sigma_n[j, i]\n",
        "            \n",
        "for i in range(dataset_size):\n",
        "    index = np.argmax(np.max(f_e[:, i:i+1], axis=1))\n",
        "    indizes.append(index)\n",
        "    \n",
        "for i in range(dataset_size):\n",
        "    failure_angle[0, i] = winkel[indizes[i]]\n",
        "\n",
        "\n",
        "X = sigma_true.T\n",
        "y = failure_angle.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id8DvnJRMZGU"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "######Data preprocessing######\n",
        "##############################\n",
        "\n",
        "#Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = Test_split_size, random_state = 0)\n",
        "#Train validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = Validation_split_size, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "X_val = sc.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJM3AU8FMZGV"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "########Neuronal network#######\n",
        "##############################\n",
        "\n",
        "regressor = Sequential()\n",
        "\n",
        "regressor.add(Dense(units = First_layer, input_dim = 5,\n",
        "                    kernel_initializer = kernel_initializer, activation = activation,\n",
        "                    kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(LeakyReLU(alpha=.1))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = Secound_layer, kernel_initializer = kernel_initializer,\n",
        "                    activation = activation, kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(LeakyReLU(alpha=.1))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = Third_layer, kernel_initializer = kernel_initializer,\n",
        "                    activation = activation, kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(LeakyReLU(alpha=.1))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = Fourth_layer, kernel_initializer = kernel_initializer,\n",
        "                    activation = activation, kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(LeakyReLU(alpha=.1))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = Fith_layer, kernel_initializer = kernel_initializer,\n",
        "                    activation = activation, kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = Sixth_layer, kernel_initializer = kernel_initializer,\n",
        "                    activation = activation, kernel_regularizer = kernel_regularizer,\n",
        "                    activity_regularizer = activity_regularizer))\n",
        "#regressor.add(Dropout(0.1))\n",
        "regressor.add(Dense(units = 1, kernel_initializer = kernel_initializer))\n",
        "\n",
        "# Compiling the ANN\n",
        "#optimizer_function = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "regressor.compile(optimizer = optimizer_function , loss = loss_function , metrics=[metric_function])\n",
        "\n",
        "callbacks = [EarlyStopping(monitor = early_stopping_monitoring, patience = early_stopping_patience)]\n",
        "\n",
        "# Check where the optimization is curried out CPU or GPU\n",
        "print(device_lib.list_local_devices())\n",
        "# Fitting the ANN to the training set\n",
        "history = regressor.fit(X_train, y_train, \n",
        "                         batch_size = 32, epochs = 200, callbacks = callbacks,\n",
        "                         validation_data=(X_val, y_val), verbose = 1)\n",
        "\n",
        "# open saved best model\n",
        "with open('trainHistoryOld', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle)\n",
        "weights = regressor.get_weights()    \n",
        "pickle.dump(regressor, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8qXxdMqMZGV"
      },
      "outputs": [],
      "source": [
        "regressor = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "with open('trainHistoryOld', 'rb') as handle: \n",
        "    lernen = load(handle)\n",
        "\n",
        "#Representation of error minimization over the epochs\n",
        "plt.plot(lernen['mean_absolute_error'], color = 'red', label = 'Mean absolute error Trainingsdaten')\n",
        "plt.plot(lernen['val_mean_absolute_error'], color = 'blue', label = 'Mean absolute error Validierungsdaten')\n",
        "plt.title('model accuracy')\n",
        "plt.legend()\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "#Predict\n",
        "y_pred = regressor.predict(X_test)\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_pred, y_test)\n",
        "r2_failure_angle = metrics.r2_score(y_test, y_pred)\n",
        "print('Der R²-Wert für den Versagenswinkel beträgt:',r2_failure_angle)\n",
        "print('Der MAE für den Versagenswinkel beträgt:',mean_absolute_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqmAIkONMZGV"
      },
      "outputs": [],
      "source": [
        "y_pred = regressor.predict(X_test)\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_pred, y_test)\n",
        "r2_failure_angle = metrics.r2_score(y_test, y_pred)\n",
        "print('Der R²-Wert für den Versagenswinkel beträgt:',r2_failure_angle)\n",
        "print('Der MAE für den Versagenswinkel beträgt:',mean_absolute_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsvxpBwDMZGW"
      },
      "outputs": [],
      "source": [
        "#for i in range(len(y_pred)):\n",
        " #   if y_pred[i]<0:\n",
        "  #      y_pred[i] = y_pred[i] + 180\n",
        "plt.scatter(y_test, y_pred, s = 0.1, color = 'blue')\n",
        "plt.title('Model')\n",
        "plt.ylabel('predicted angle')\n",
        "plt.xlabel('correct angle')\n",
        "plt.show()\n",
        "mean_absolute_error = mean_absolute_error_eval(y_pred, y_test)\n",
        "error = np.empty([len(y_pred),1])\n",
        "for i in range(len(y_pred)):\n",
        "    error[i] = min(min(np.abs(y_pred[i] - y_test[i]),\n",
        "                                  np.abs(y_pred[i] - (180+y_test[i]))),\n",
        "                                  np.abs(y_test[i] - (180+y_pred[i])))\n",
        "r2_failure_angle = metrics.r2_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWN-vuQAMZGW"
      },
      "outputs": [],
      "source": [
        "kernel_initializer = ['he_uniform', 'lecun_uniform']\n",
        "number_of_neurons = [60, 65, 70]\n",
        "number_of_hidden_layers = [7, 8, 9, 10]\n",
        "optimizer = ['rmsprop', 'adagrad', 'adam']\n",
        "loss = [mean_absolute_error_180, mean_sqared_error_180]\n",
        "param_list = grid_search(optimizer = optimizer, kernel_initializer = kernel_initializer, number_of_hidden_layers = number_of_hidden_layers, loss = loss, number_of_neurons = number_of_neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ-z8jQ-MZGW"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# Start of gred search#\n",
        "#######################\n",
        "\n",
        "columns = ['number of neurons', 'kernel initializer', 'optimizer', 'loss function',\n",
        "           'batch sizes', 'number of hidden layers', 'MAE failure angle']\n",
        "grid_search_list = pd.DataFrame(param_list, columns = columns)\n",
        "grid_search_list = grid_search_list.sort_values(by='MAE failure angle')\n",
        "grid_search_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvvULmwRMZGZ"
      },
      "outputs": [],
      "source": [
        "def grid_search(**keyword_parameters):\n",
        "    \n",
        "    #Keywords abfragen und übernhemen/ Falls nciht vorhanden defaults setzen\n",
        "    \n",
        "    complete_time = time.time()\n",
        "    if ('number_of_neurons' in keyword_parameters):\n",
        "        number_of_neuron = number_of_neurons\n",
        "    else:\n",
        "        number_of_neuron = [4]\n",
        "        \n",
        "    if ('kernel_initializer' in keyword_parameters):\n",
        "        kernel_initializers = kernel_initializer\n",
        "    else:\n",
        "        kernel_initializers = ['lecun_uniform']\n",
        "        \n",
        "    if ('optimizer' in keyword_parameters):\n",
        "        optimizers = optimizer\n",
        "    else:\n",
        "        optimizers = ['adam']\n",
        "        \n",
        "    if ('loss' in keyword_parameters):\n",
        "        losses = loss\n",
        "    else:\n",
        "        losses = ['logcosh']        \n",
        "        \n",
        "    if ('number_of_hidden_layers' in keyword_parameters):\n",
        "        number_of_hidden_layer = number_of_hidden_layers\n",
        "    else:\n",
        "        number_of_hidden_layer = [1]\n",
        "\n",
        "    if ('batch_size' in keyword_parameters):\n",
        "        batch_sizes = batch_size\n",
        "    else:\n",
        "        batch_sizes = [32]\n",
        "        \n",
        "    \n",
        "    number_of_runs = len(number_of_neuron)*len(kernel_initializers)*len(optimizers)*len(losses)*len(number_of_hidden_layer)*len(batch_sizes)\n",
        "    param_list = np.empty([number_of_runs, 7], dtype = 'U25')\n",
        "    run_counter = -1\n",
        "    for a in range(len(number_of_neuron)):\n",
        "        for b in range(len(kernel_initializers)):\n",
        "            for c in range(len(optimizers)):\n",
        "                for d in range(len(losses)):\n",
        "                    for e in range(len(batch_sizes)):\n",
        "                        hidden_layer_counter = number_of_hidden_layer[0] - 2\n",
        "                        for f in range(len(number_of_hidden_layer)):\n",
        "                            one_loop_time = time.time()\n",
        "                            hidden_layer_counter = hidden_layer_counter + 1\n",
        "                            run_counter = run_counter + 1\n",
        "                            regressor = 0\n",
        "                            regressor = Sequential()\n",
        "                            regressor.add(Dense(units = number_of_neuron[a], input_dim = 5,\n",
        "                                      kernel_initializer = kernel_initializers[b], activation = activation,\n",
        "                                      kernel_regularizer = kernel_regularizer,\n",
        "                                      activity_regularizer = activity_regularizer))\n",
        "                            \n",
        "                            for g in range(hidden_layer_counter):\n",
        "                                regressor.add(Dense(units = number_of_neuron[a],\n",
        "                                        kernel_initializer = kernel_initializers[b], activation = activation,\n",
        "                                        kernel_regularizer = kernel_regularizer,\n",
        "                                        activity_regularizer = activity_regularizer))\n",
        "                            regressor.add(Dense(units = 1, kernel_initializer = kernel_initializers[b]))\n",
        "                            regressor.compile(optimizer = optimizers[c] , loss = losses[d] , metrics=[metric_function])\n",
        "                            #plot_model(regressor, to_file='model'+str(run_counter)+'.png')\n",
        "                            callbacks = [EarlyStopping(monitor = early_stopping_monitoring, patience = early_stopping_patience)]\n",
        "                            history = regressor.fit(X_train, y_train, \n",
        "                                                    batch_size = batch_sizes[e], epochs = 500, \n",
        "                                                    callbacks=callbacks, \n",
        "                                                    validation_data=(X_val, y_val), verbose = 1)\n",
        "                            y_pred = regressor.predict(X_test)\n",
        "                            mean_absolute_error = mean_absolute_error_eval(y_pred, y_test)\n",
        "                            \n",
        "                            #Saving the results\n",
        "                            param_list[run_counter, 0] = number_of_neuron[a]\n",
        "                            param_list[run_counter, 1] = kernel_initializers[b]\n",
        "                            param_list[run_counter, 2] = optimizers[c]\n",
        "                            param_list[run_counter, 3] = losses[d]\n",
        "                            param_list[run_counter, 4] = batch_sizes[e]\n",
        "                            param_list[run_counter, 5] = hidden_layer_counter + 1\n",
        "                            param_list[run_counter, 6] = mean_absolute_error\n",
        "                            #pickle.dump(regressor, open('Gewichte Durchlauf'+str(run_counter)+'.sav', 'wb'))\n",
        "                            print('Durchlauf', run_counter + 1, 'von', len(param_list))\n",
        "                            elapsed_time = time.time() - one_loop_time\n",
        "                            print('Zeit für letzten loop:', elapsed_time)\n",
        "                            \n",
        "    complete_run_time = time.time() - complete_time\n",
        "    print('time for grid search:', complete_run_time)                        \n",
        "    return param_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm3dyirgMZGa"
      },
      "outputs": [],
      "source": [
        "kernel_initializer = ['he_uniform']\n",
        "number_of_neurons = [25, 30, 35, 40, 45]\n",
        "number_of_hidden_layers = [5, 6]\n",
        "loss = [huber_loss, 'logcosh', 'mae', 'mape', 'mse']\n",
        "param_list = grid_search(kernel_initializer = kernel_initializer, number_of_hidden_layers = number_of_hidden_layers, loss = loss, number_of_neurons = number_of_neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXGxO9EiMZGa"
      },
      "outputs": [],
      "source": [
        "columns = ['number of neurons', 'kernel initializer', 'optimizer', 'loss function',\n",
        "           'batch sizes', 'number of hidden layers', 'R² failure angle', 'MAE failure angle']\n",
        "grid_search_list = pd.DataFrame(param_list, columns = columns)\n",
        "grid_search_list = grid_search_list.sort_values(by='MAE failure angle')\n",
        "grid_search_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtLfNGV4MZGa"
      },
      "outputs": [],
      "source": [
        "kernel_initializer = ['he_uniform']\n",
        "number_of_neurons = [45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 110, 120, 130, 140, 150]\n",
        "number_of_hidden_layers = [6]\n",
        "param_list = grid_search(kernel_initializer = kernel_initializer, number_of_hidden_layers = number_of_hidden_layers, number_of_neurons = number_of_neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWZXGtxHMZGb"
      },
      "outputs": [],
      "source": [
        "columns = ['number of neurons', 'kernel initializer', 'optimizer', 'loss function',\n",
        "           'batch sizes', 'number of hidden layers', 'R² failure angle', 'MAE failure angle']\n",
        "grid_search_list = pd.DataFrame(param_list, columns = columns)\n",
        "grid_search_list = grid_search_list.sort_values(by='MAE failure angle')\n",
        "grid_search_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_Cbk2bpMZGb"
      },
      "outputs": [],
      "source": [
        "kernel_initializer = ['he_uniform']\n",
        "number_of_neurons = [50, 51, 52, 53, 54, 55]\n",
        "number_of_hidden_layers = [6]\n",
        "param_list = grid_search(kernel_initializer = kernel_initializer, number_of_hidden_layers = number_of_hidden_layers, number_of_neurons = number_of_neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KyVLpsjMZGb"
      },
      "outputs": [],
      "source": [
        "columns = ['number of neurons', 'kernel initializer', 'optimizer', 'loss function',\n",
        "           'batch sizes', 'number of hidden layers', 'R² failure angle', 'MAE failure angle']\n",
        "grid_search_list = pd.DataFrame(param_list, columns = columns)\n",
        "grid_search_list = grid_search_list.sort_values(by='MAE failure angle')\n",
        "grid_search_list"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Puck_failure_2-checkpoint.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}